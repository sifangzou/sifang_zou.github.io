
@inproceedings{zou_generation_2018,
	address = {Beijing},
	title = {Generation textured contact lenses iris images based on {4DCycle}-{GAN}},
	isbn = {978-1-5386-3788-3},
	url = {https://ieeexplore.ieee.org/document/8546154/},
	doi = {10.1109/ICPR.2018.8546154},
	abstract = {With the development of iris recognition, many identity authentication applications began to use this inherent biometric ID. Despite the breakthroughs in the identiﬁcation with iris recognition technology, one primary problem remains unsolved: the presentation spoof attack. In this paper, we present a novel algorithm 4DCycle-GAN for expanding the spoof iris image database by synthesizing fake iris images wearing textured contact lenses. The proposed 4DCycle-GAN follows the CycleConsistent Adversarial Networks (Cycle-GAN) framework which translating between one kind images (genuine iris images) and one other kind images (textured contact lenses iris images). The 4DCycle-GAN introduces two more discriminators to improve the Cycle-GAN at the defect of lack of diversity. The two new discriminators ’prefer’ images generated by the generators, while the original discriminators in Cycle-GAN ’prefer’ real captured images. These new added confrontations make the 4DCycle-GAN avoid generating a certain kind of contact lenses texture which is larger percentage of the training iris database. The synthesized textured contact lenses iris images are used for spooﬁng iris detection training to improve the robustness of classiﬁcation algorithm. Both the Cycle-GAN and the 4DCycle-GAN synthesizing images can improve the spoof classiﬁcation results. Moreover, by using the 4DCycle-GAN, the spoof classiﬁcation results are distinctly improved for unrelated non-homologous database experiments. Extensive experimental results show that the proposed method can improve the anti-spoof ability of iris recognition system.},
	language = {en},
	urldate = {2024-05-07},
	booktitle = {2018 24th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	publisher = {IEEE},
	author = {Zou, Hang and Zhang, Hui and Li, Xingguang and Liu, Jing and He, Zhaofeng},
	month = aug,
	year = {2018},
	pages = {3561--3566},
	file = {Zou 等 - 2018 - Generation Textured Contact Lenses Iris Images Based on 4DCycle-GAN.pdf:C\:\\Users\\hp\\Zotero\\storage\\PHAEP454\\Zou 等 - 2018 - Generation Textured Contact Lenses Iris Images Based on 4DCycle-GAN.pdf:application/pdf},
}

@article{zhang_mmnerf_2023,
	title = {{MMNeRF}: {Multi}-modal and multi-view optimized cross-scene neural radiance fields},
	volume = {11},
	copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
	issn = {2169-3536},
	shorttitle = {{MMNeRF}},
	url = {https://ieeexplore.ieee.org/document/10064260/},
	doi = {10.1109/ACCESS.2023.3254548},
	abstract = {We present MMNeRF, a simple yet powerful learning framework for highly photo-realistic novel view synthesis by learning Multi-modal and Multi-view features to guide neural radiance fields to a generic model. Novel view synthesis has achieved great improvement with the significant success of NeRFseries methods. However, how to make the method generic across scenes has always been a challenging task. A good idea is to introduce 2D image features as prior knowledge for adaptive modeling, yet RGB features lack geometry and 3D spatial information, which causes shape-radiance ambiguity issues and lead to blurry and low-resolution results in the synthesis images. We propose a multi-modal multi-view method to make up for the existing methods. Specifically, we introduce depth features besides RGB features into the model and effectively fuse these multi-modal features by modality-based attention. Furthermore, Our framework innovatively adopts the transformer encoder to fuse multi-view features and uses the transformer decoder to adaptively incorporate the target view with global memory. Extensive experiments are carried out on both categories-specific and category-agnostic benchmarks, and the results demonstrate that our MMNeRF achieves state-of-the-art neural rendering performance.},
	language = {en},
	urldate = {2024-05-07},
	journal = {IEEE Access},
	author = {Zhang, Qi and Wang, Bo Han and Yang, Ming Chuan and Zou, Hang},
	year = {2023},
	note = {JCR分区: Q2
中科院分区升级版: 计算机科学3区
影响因子: 3.9
5年影响因子: 4.1
EI: 是},
	pages = {27401--27413},
}

@inproceedings{zhang_cdnerf_2022,
	address = {Cham},
	title = {{CDNeRF}: {A} multi-modal feature guided neural radiance fields},
	isbn = {978-3-031-20497-5},
	abstract = {We present CDNeRF, a simple yet powerful learning framework that creates novel view synthesis by reconstructing neural radiance fields from a single view RGB image. Novel view synthesis by neural radiance fields has achieved great improvement with the development of deep learning. However, how to make the method generic across scenes has always been a challenging task. A good idea is to introduce 2D image features as prior knowledge for adaptive modeling, yet RGB features (C) lack geometry and 3D spacial information. To compensate, we introduce depth features into the model. Our method uses a variant depth estimation network to extract depth features (D) without the need for additional input. In addition, we also introduce the transformer module to effectively fuse the multi-modal features of RGB and depth. Extensive experiments are carried out on two categories specific benchmarks (i.e., Chair, Car) and two category agnostic benchmarks (i.e., ShapeNet, DTU). The results demonstrate that our CDNeRF outperforms the previous methods, and achieves state-of-the-art neural rendering performance.},
	booktitle = {Artificial {Intelligence}},
	publisher = {Springer Nature Switzerland},
	author = {Zhang, Qi and Liu, Qiaoqiao and Zou, Hang},
	editor = {Fang, Lu and Povey, Daniel and Zhai, Guangtao and Mei, Tao and Wang, Ruiping},
	year = {2022},
	pages = {204--215},
}

@inproceedings{zhang_coarse--fine_2017,
	address = {Cham},
	title = {Coarse-to-fine iris recognition based on multi-variant ordinal measures feature complementarity},
	isbn = {978-3-319-69923-3},
	abstract = {Iris recognition inevitably need to tackle extremely large scale database matching issue which challenges the iris recognition in both computing efficiency and accuracy. As a feasible solution, the iris image classification has great potential and needs further studies. We propose a multi-variant Ordinal Measures feature complementarity based coarse-to-fine iris recognition strategy. Two OM variant feature are proposed for iris classification. One is very large scale OM feature (VLSOM), and the other is histogram statistics of OM Run-Length Coding (HOMRLC). VLSOM, HOMRLC and OM describes overall appearance, global statistic and local characteristics of iris respectively. Extensive experiments show advantages of the proposed complementarity feature.},
	booktitle = {Biometric {Recognition}},
	publisher = {Springer International Publishing},
	author = {Zhang, Hui and Zhang, Man and He, Zhaofeng and Zou, Hang and Wang, Rui},
	editor = {Zhou, Jie and Wang, Yunhong and Sun, Zhenan and Xu, Yong and Shen, Linlin and Feng, Jianjiang and Shan, Shiguang and Qiao, Yu and Guo, Zhenhua and Yu, Shiqi},
	year = {2017},
	pages = {411--419},
}
